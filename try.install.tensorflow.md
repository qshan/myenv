
# python3 -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"


- install tensorflow
```bash
cd /data1/tools/python
python3.m.venv tensorflow
source  tensorflow/bin/activate
pip install tensorflow
```

-tensorflow 2
```bash
# Requires the latest pip
pip install --upgrade pip

# Current stable release for CPU and GPU
pip install tensorflow

## # Or try the preview build (unstable)
## pip install tf-nightly
```


- reference: https://tensorflow.google.cn/lite/guide/python?hl=zh-cn
- install tensorflow
## Current stable release for CPU and GPU
python3 -m pip install tensorflow
```bash

## conda create -p /data1/work/tinyml -yn tf-lite
conda create -p /data1/work/tinyml/conda-tf-lite python=3.10.0 numpy=1.26.0
conda activate /data1/work/tinyml/conda-tf-lite

python3 -m pip install tflite-runtime

## Current stable release for CPU and GPU
python3 -m pip install tensorflow

#??#pip install tflite-model-maker

## https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/hello_world

```

```bash

git clone https://github.com/tensorflow/tflite-micro.git
cd tflite-micro

git config --global url."https://github.com/google/pigweed.git".insteadOf "https://pigweed.googlesource.com/pigweed/pigweed"

git.submodule.update.f

pip3 uninstall numpy
conda search numpy
conda install numpy=2.1.3
#pip install --upgrade numpy

pip install pillow

#### Run the tests on a development machine
# run it using make
make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test
```

- Try hello_world
```bash

# reference : https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world
# https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world/train/README.md
# https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world/README.md

#### Run the evaluate.py script on a development machine
# The evaluate.py script runs the hello_world.tflite model with x_values in the range of [0, 2*PI]. The script plots a diagram of the predicted value of sinwave using TFLM interpreter and compare that prediction with the actual value generated by the numpy lib.
# wget -c https://github.com/bazelbuild/bazel/releases/download/7.0.0/bazel-7.0.0-installer-linux-x86_64.sh
# sudo cp /usr/local/lib/bazel/bin/bazel /usr/local/lib/bazel/bin/bazel-7.0.0-linux-x86_64
bazel build tensorflow/lite/micro/examples/hello_world:evaluate
bazel run tensorflow/lite/micro/examples/hello_world:evaluate
bazel run tensorflow/lite/micro/examples/hello_world:evaluate -- --use_tflite

#### Run the evaluate_test.py script on a development machine
bazel build tensorflow/lite/micro/examples/hello_world:evaluate_test
bazel run tensorflow/lite/micro/examples/hello_world:evaluate_test

#### Run the tests on a development machine
# Run the cc test using bazel
??bazel run tensorflow/lite/micro/examples/hello_world:hello_world_test
#good#And to run it using make
make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test

#### Train your own model
#So far you have used an existing trained model to run inference on microcontrollers. If you wish to train your own model, here are the scripts that can help you to achieve that.
bazel build tensorflow/lite/micro/examples/hello_world:train

#And to run it
bazel-bin/tensorflow/lite/micro/examples/hello_world/train --save_tf_model --save_dir=/tmp/model_created/

#The above script will create a TF model and TFlite model inside the /tmp/model_created directory.

#Now the above model is a float model. Means it can take floating point input and can produce floating point output.

#### If we want a fully quantized model we can use the ptq.py script inside the quantization directory. The ptq.py script can take a floating point TF model and can produce a quantized model.

#Build the ptq.py script like
bazel build tensorflow/lite/micro/examples/hello_world/quantization:ptq

#Then we can run the ptq script to convert the float model to quant model as follows. Note that we are using the directory (/tmp/model_created) of the TF model as the source_model_dir here. The quant model (named hello_world_int8.tflite) will be created inside the target_dir. The ptq.py script will convert the TF model found inside the /tmp/model_created folder and convert it to a int8 TFlite model.
bazel-bin/tensorflow/lite/micro/examples/hello_world/quantization/ptq --source_model_dir=/tmp/model_created --target_dir=/tmp/quant_model/

```
